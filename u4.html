<HTML>
  <BODY bgcolor=#39101a>
    <h2 align="center"> UNIDAD 4 </h3><br>
      <table bgcolor=#35141c align="Center" border="1" cellpadding="1">
	<tr>
	<td><a href="principal.html">VOLVER A PAGINA PRINCIPAL </a></td>
	<td><a href="u1.html">| IR A UNIDAD 1 |</a></td>
	<td><a href="u2.html">| IR A UNIDAD 2 |</a></td>
	<td><a href="u3.html">| IR A UNIDAD 3 |</a></td>
	</tr>
       </table><br>
      <table align="center"><tr>
       <td><img src="its.png" width="100px" height="100px"> </td>
       <td><ol> 
       <li><a href="u4.html#abcp"> Aspectos Basicos de la computacion paralela </a></li>
	  <li><a href="u4.html#tcp"> Tipos de computacion paralela </a></li>
           <ol> 
             <li><a href="u4.html#c"> Clasificacion </a></li>
	     <li><a href="u4.html#acs"> Arquitectura de computadoras secuenciales </a></li> 
	     <li><a href="u4.html#odm"> Organizacion de direcciones de memoria </a></li> 
           </ol> 
	   <li><a href="u4.html#smm"> Sistema de memoria (Compartida) Multiprocesadores </a></li>
           <ol> 
             <li><a href="u4.html#rid"> Redes de interconexion dinamica (indirecta) Medio compartido conmutadas </a></li> 
           </ol> 
	   <li><a href="u4.html#smdm"> Sistemas de memoria distribuida Multicomputadores </a></li>
	   <ol> 
             <li><a href="u4.html#rie"> Redes de interconexion estaticas </a></li>
           </ol> 
	   <li><a href="u4.html#cpe"> Casos para estudios </a></li>
      </ol></td>
     <td><img src="TecNM-logo.png" width="200px" height="100px"> </td>
     </tr></table><br><br><br>
     <table border="1" cellpadding="1"> <tr>
  <td> <p><font color=#39101a> ~~~~~~~~~~~~~~~~~~~~~~~~~</font></p></td>
  <td bgcolor=#c6efe5>
    <h2 align="center"><a name="abcp"> Aspectos basicos de la computacion paralela </a></h2>
    <p>
	La computación paralela es una forma de cómputo en la que muchas instrucciones se 
	ejecutan simultáneamente, operando sobre el principio de que problemas grandes, a menudo se pueden 
	dividir en unos más pequeños, que luego son resueltos simultáneamente (en paralelo). 
    </p>
    <h2 align="center"><a name="tcp"> Tipos de computacion paralela </a></h2>
    <p>
	</p><h4>Paralelismo a nivel de bit.</h4><p>
	Desde el advenimiento de la integración a gran escala (VLSI) como
	tecnología de fabricación de chips de computadora en la década de
	1970 hasta alrededor de 1986, la aceleración en la arquitectura de
	computadores se lograba en gran medida duplicando el tamaño de la
	palabra en la computadora, la cantidad de información que el
	procesador puede manejar por ciclo. El aumento del tamaño de la
	palabra reduce el número de instrucciones que el procesador debe
	ejecutar para realizar una operación en variables cuyos tamaños son
	mayores que la longitud de la palabra. Por ejemplo, cuando un
	procesador de 8 bits debe sumar dos enteros de 16 bits, el
	procesador primero debe adicionar los 8 bits de orden inferior de
	cada número entero con la instrucción de adición, a continuación,
	añadir los 8 bits de orden superior utilizando la instrucción de
	adición con acarreo que tiene en cuenta el bit de acarreo de la
	adición de orden inferior, en este caso un procesador de 8 bits
	requiere dos instrucciones para completar una sola operación, en
	donde un procesador de 16 bits necesita una sola instrucción para
	poder completarla.
	</p><h4>Paralelismo a nivel de instrucción.</h4><p>
	Los procesadores modernos tienen ''pipeline'' de instrucciones de
	varias etapas. Cada etapa en el pipeline corresponde a una acción
	diferente que el procesador realiza en la instrucción correspondiente
	a la etapa; un procesador con un pipeline de N etapas puede tener
	hasta n instrucciones diferentes en diferentes etapas de finalización.
	El ejemplo canónico de un procesador segmentado es un procesador
	RISC, con cinco etapas: pedir instrucción, decodificar, ejecutar,
	acceso a la memoria y escritura. El procesador Pentium 4 tenía un
	pipeline de 35 etapas.
	</p><h4>Paralelismo de datos.</h4><p>
	El paralelismo de datos es el paralelismo inherente en programas
	con ciclos, que se centra en la distribución de los datos entre los
	diferentes nodos computacionales que deben tratarse en paralelo.
	"La paralelización de ciclos conduce a menudo a secuencias
	similares de operaciones —no necesariamente idénticas— o
	funciones que se realizan en los elementos de una gran estructura de
	datos". Muchas de las aplicaciones científicas y de ingeniería
	muestran paralelismo de datos.
	Una dependencia de terminación de ciclo es la dependencia de una
	iteración de un ciclo en la salida de una o más iteraciones anteriores.
	Las dependencias de terminación de ciclo evitan la paralelización de
	ciclos.
	</p><h4>Paralelismo de tareas.</h4><p>
	Paralelismo de tareas es un paradigma de la programación
	concurrente que consiste en asignar distintas tareas a cada uno de los
	procesadores de un sistema de cómputo. En consecuencia, cada
	procesador efectuará su propia secuencia de operaciones.<br>
	En su modo más general, el paralelismo de tareas se representa
	mediante un grafo de tareas, el cual es subdividido en subgrafos que
	son luego asignados a diferentes procesadores. De la forma como se
	corte el grafo, depende la eficiencia de paralelismo resultante. La
	partición y asignación óptima de un grafo de tareas para ejecución
	concurrente es un problema NP-completo, por lo cual en la práctica
	se dispone de métodos heurísticos aproximados para lograr una
	asignación cercana a la óptima.
    </p>
    <h3><a name="c"> Clasificacion </a></h3>
    <p>
	Las computadoras paralelas se pueden clasificar de acuerdo con el
	nivel en el que el hardware soporta paralelismo. Esta clasificación es
	análoga a la distancia entre los nodos básicos de cómputo. Estos no
	son excluyentes entre sí, por ejemplo, los grupos de
	multiprocesadores simétricos son relativamente comunes.
	<ul>
	<li>Computación multinúcleo: un procesador multinúcleo es un
	procesador que incluye múltiples unidades de ejecución
	(núcleos) en el mismo chip. Un procesador multinúcleo puede
	ejecutar múltiples instrucciones por ciclo de secuencias de
	instrucciones múltiples.</li>
	<li>Multiprocesamiento simétrico: un multiprocesador simétrico
	(SMP) es un sistema computacional con múltiples
	procesadores idénticos que comparten memoria y se conectan
	a través de un bus. La contención del bus previene el escalado
	de esta arquitectura.</li>
	<li>Computación en clúster: un clúster es un grupo de
	ordenadores débilmente acoplados que trabajan en estrecha
	colaboración, de modo que en algunos aspectos pueden
	considerarse como un solo equipo.</li>
	<li>Procesamiento paralelo masivo: tienden a ser más grandes
	que los clústeres, con «mucho más» de 100 procesadores. En
	un MPP, cada CPU tiene su propia memoria y una copia del
	sistema operativo y la aplicación.</li>
	<li>Computación distribuida: la computación distribuida es la
	forma más distribuida de la computación paralela. Se hace uso
	de ordenadores que se comunican a través de la Internet para
	trabajar en un problema dado.</li>
	<li>Computadoras paralelas especializadas: dentro de la
	computación paralela, existen dispositivos paralelos
	especializados que generan interés. Aunque no son específicos
	para un dominio, tienden a ser aplicables sólo a unas pocas
	clases de problemas paralelos.</li>
	<li>Cómputo reconfigurable con arreglos de compuertas
	programables: el cómputo reconfigurable es el uso de un
	arreglo de compuertas programables (FPGA) como
	coprocesador de un ordenador de propósito general.</li>
	<li>Cómputo de propósito general en unidades de
	procesamiento gráfico (GPGPU): es una tendencia
	relativamente reciente en la investigación de ingeniería
	informática. Los GPUs son co-procesadores que han sido
	fuertemente optimizados para procesamiento de gráficos por
	computadora.</li>
	<li>Circuitos integrados de aplicación específica: debido a que
	un ASIC (por definición) es específico para una aplicación
	dada, puede ser completamente optimizado para esa
	aplicación. Como resultado, para una aplicación dada, un
	ASIC tiende a superar a un ordenador de propósito general.</li>
	<li>Procesadores vectoriales: pueden ejecutar la misma
	instrucción en grandes conjuntos de datos. Tienen operaciones
	de alto nivel que trabajan sobre arreglos lineales de números o
	vectores.</li>
	</ul>
    </p>
    <h3><a name="acs"> Arquitectura de computadoras secuenciales </a></h3>
    <p>
	A diferencia de los sistemas combinacionales, en los sistemas
	secuenciales, los valores de las salidas, en un momento dado, no
	dependen exclusivamente de los valores de las entradas en dicho
	momento, sino también dependen del estado anterior o estado
	interno. El sistema secuencial más simple es el biestable, de los
	cuales, el de tipo D (o cerrojo) es el más utilizado actualmente.<br>
	El sistema secuencial requiere de la utilización de un dispositivo de
	memoria que pueda almacenar la historia pasada de sus entradas
	(denominadas variables de estado) y le permita mantener su estado
	durante algún tiempo, estos dispositivos de memoria pueden ser
	sencillos como un simple retardador o celdas de memoria de tipo
	DRAM, SRAM o multivibradores biestables también conocido
	como Flip-Flop.
    </p>
    <h3><a name="odm"> Organizacion de direccion de memoria </a></h3>
    <p>
	La memoria principal en un ordenador en paralelo puede ser
	compartida —compartida entre todos los elementos de
	procesamiento en un único espacio de direcciones—, o distribuida
	—cada elemento de procesamiento tiene su propio espacio local de
	direcciones—. El término memoria distribuida se refiere al hecho de
	que la memoria se distribuye lógicamente, pero a menudo implica
	que también se distribuyen físicamente. La memoria distribuida-
	compartida y la virtualización de memoria combinan los dos
	enfoques, donde el procesador tiene su propia memoria local y
	permite acceso a la memoria de los procesadores que no son locales.
	Los accesos a la memoria local suelen ser más rápidos que los
	accesos a memoria no local.<br>
	Las arquitecturas de ordenador en las que cada elemento de la
	memoria principal se puede acceder con igual latencia y ancho de
	banda son conocidas como arquitecturas de acceso uniforme a
	memoria (UMA). Típicamente, sólo se puede lograr con un sistema
	de memoria compartida, donde la memoria no está distribuida
	físicamente. Un sistema que no tiene esta propiedad se conoce como
	arquitectura de acceso a memoria no uniforme (NUMA). Los
	sistemas de memoria distribuidos tienen acceso no uniforme a la
	memoria.
    </p>
    <h2 align="center"><a name="smm"> Sistema de memoria (Compartida) Multiprocesadores </a></h2>
    <p>
	<ul>
	<li>Todos los procesadores acceden a una memoria común.</li>
	<li>La comunicación entre procesadores se hace a través de la
	memoria.</li>
	<li>Se necesitan primitivas de sincronismo para asegurar el
	intercambio de datos.</li>
	</ul>
	</p><h4>Estructura de los multiprocesadores de memoria compartida.</h4><p>
	La mayoría de los multiprocesadores comerciales son del tipo UMA
	(Uniform Memory Access): todos los procesadores tienen igual
	tiempo de acceso a la memoria compartida. En la arquitectura UMA
	los procesadores se conectan a la memoria a través de un bus, una
	red multietapa o un conmutador de barras cruzadas (red multietapa o
	un conmutador de barras cruzadas (crossbar crossbar) y disponen de
	su propia ) y disponen de su propia memoria caché. Los
	procesadores tipo NUMA (Non Uniform Memory Access) presentan
	tiempos de acceso a la memoria compartida que dependen de la
	ubicación del elemento de proceso y la memoria.
    </p>
    <h3><a name="rid"> Redes de interconexion dinamica (Indirecta) Medio compartido conmutadas </a></h3>
    <p>
	</p><h4>Conexión por bus compartido.</h4><p>
	Es la organización más común en los computadores personales y
	servidores.<br>
	El bus consta de líneas de dirección, datos y control para
	implementar:
	<ul>
	<li>El protocolo de transferencias de datos con la memoria.</li>
	<li>El arbitraje del acceso al bus cuando más de un procesador
	compite por utilizarlo.</li>
	</ul>
	Los procesadores utilizan cachés locales para:
	<ul>
	<li>Reducir el tiempo medio de acceso a memoria, como en un
	monoprocesador.</li>
	<li>Disminuir la utilización del bus compartido.</li>	
	</ul>
	</p><h4>Conmutadas.</h4><p>
	</p><h4>Conexión por conmutadores crossbar.</h4><p>
	Cada procesador (Pi) y cada módulo de memoria (Mi) tienen su
	propio bus. Existe un conmutador (S) en los puntos de intersección
	que permite conectar un bus de memoria con un bus de procesador.
	Para evitar conflictos cuando más de un procesador pretende acceder
	al mismo módulo de memoria se establece un orden de prioridad. Se
	trata de una red sin bloqueo con una conectividad completa pero de
	alta complejidad.
    </p>
    <h2 align="center"><a name="smdm"> Sistemas de memoria distribuida Multicomputadores </a></h2>
    <p>
	Cada procesador tiene su propia memoria y la comunicación se
	realiza por intercambio explícito de mensajes a través de una red.<br>

	</p><h4>Ventajas</h4><p>
	<ul>
	<li>El número de nodos puede ir desde algunas decenas hasta
	varios miles (o más).</li>
	<li>La arquitectura de paso de mensajes tiene ventajas sobre la de
	memoria compartida cuando el número de procesadores es
	grande.</li>
	<li>El número de canales físicos entre nodos suele oscilar entre
	cuatro y ocho.</li>
	<li>Esta arquitectura es directamente escalable y presenta un bajo
	coste para sistemas grandes.</li>
	<li>Un problema se especifica como un conjunto de procesos que
	se comunican entre sí y que se hacen corresponder sobre la
	estructura física de procesadores.</li>
	</ul>
	</p><h4>Desventajas</h4><p>
	<ul>
	<li>Se necesitan técnicas de sincronización para acceder a las
	variables compartidas.</li>
	<li>La contención en la memoria puede reducir significativamente
	la velocidad.</li>
	<li>No son fácilmente escalables a un gran número de
	procesadores.</li>
	</ul>
    </p>
    <h3><a name="rie"> Redes de interconexion estaticas </a></h3>
    <p>
	Los multicomputadores utilizan redes estáticas con enlaces directos
	entre nodos. Cuando un nodo recibe un mensaje lo procesa si viene
	dirigido a dicho nodo. Si el mensaje no va dirigido al nodo receptor
	lo reenvía a otro por alguno de sus enlaces de salida siguiendo un
	protocolo de encaminamiento.
	</p><h4>Propiedades más significativas</h4><p>
	<ul>
	<li>Topología de la red: determina el patrón de interconexión
	entre nodos.</li>
	<li>Diámetro de la red: distancia máxima de los caminos más
	cortos entre dos nodos de la red.</li>
	<li>Latencia: retardo de tiempo en el peor caso para un mensaje
	transferido a través de la red.</li>
	<li>Ancho de banda: Transferencia máxima de datos en
	Mbytes/segundo.</li>
	<li>Escalabilidad: posibilidad de expansión modular de la red.</li>
	<li>Grado de un nodo: número de enlaces o canales que inciden
	en el nodo.</li>
	<li>Algoritmo de encaminamiento: determina el camino que
	debe seguir un mensaje desde el nodo emisor al nodo receptor.</li>
	</ul>
    </p>
    <h2 align="center"><a name="cpe"> Casos para estudios </a></h2>
    <p>
	Por numerosos motivos, el procesamiento distribuido se ha
	convertido en un área de gran importancia e interés dentro de la
	ciencia de la computación, produciendo profundas transformaciones
	en las líneas de investigación y desarrollo.<br>
	Interesa realizar investigación en la especificación, transformación,
	optimización y evaluación de algoritmos distribuidos y paralelos.
	Esto incluye el diseño y desarrollo de sistemas paralelos, la
	transformación de algoritmos secuenciales en paralelos, y las
	métricas de evaluación de performance sobre distintas plataformas
	de soporte (hardware y software). Más allá de las mejoras constantes
	en las arquitecturas físicas de soporte, uno de los mayores desafíos
	se centra en cómo aprovechar al máximo la potencia de las mismas.<br>

	</p><h4>Líneas de investigación y desarrollo</h4><p>
	<ul>
	<li>Paralelización de algoritmos secuenciales. Diseño y
	optimización de algoritmos.</li>
	<li>Arquitecturas multicore y multithreading en multicore.</li>
	<li>Modelos de representación y predicción de performance de
	algoritmos paralelos.</li>
	<li>Mapping y scheduling de aplicaciones paralelas sobre distintas
	arquitecturas multiprocesador.</li>
	<li>Métricas del paralelismo. Speedup, eficiencia, rendimiento,
	granularidad, superlinealidad.</li>
	<li>Balance de carga estático y dinámico. Técnicas de balanceo de
	carga.</li>
	<li>Análisis de los problemas de migración y asignación óptima de
	procesos y datos a procesadores.</li>
	<li>Patrones de diseño de algoritmos paralelos.</li>
	<li>Escalabilidad de algoritmos paralelos en arquitecturas
	multiprocesador distribuidas.</li>
	<li>Implementación de soluciones sobre diferentes modelos de
	arquitectura homogéneas y heterogéneas.</li>
	<li>Laboratorios remotos para el acceso transparente a recursos de
	cómputo paralelo.</li>
	</ul>
    </p>
  </td>
  <td><p><font color=#39101a> ~~~~~~~~~~~~~~~~~~~~~~~~~</font></p></td></tr></table><br>
      <table bgcolor=#35141c align="Center" border="1" cellpadding="1">
	<tr>
	<td><a href="principal.html">VOLVER A PAGINA PRINCIPAL </a></td>
	<td><a href="u1.html">| IR A UNIDAD 1 |</a></td>
	<td><a href="u2.html">| IR A UNIDAD 2 |</a></td>
	<td><a href="u3.html">| IR A UNIDAD 3 |</a></td>
	</tr>
       </table>
  </BODY>
</HTML>